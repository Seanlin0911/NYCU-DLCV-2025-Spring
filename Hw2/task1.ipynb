{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11376212,"sourceType":"datasetVersion","datasetId":7122296},{"sourceId":336525,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":281652,"modelId":302532}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Basic imports\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# PyTorch imports\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision.transforms import functional as F\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.backbone_utils import BackboneWithFPN\nfrom torchvision.ops.feature_pyramid_network import LastLevelMaxPool\n\n# Utilities for bounding boxes\nimport torchvision.transforms as T\nfrom torchvision.ops import nms\n\n# Progress bar\nfrom tqdm import tqdm\n\n# Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nprint(\"import finish\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:42:33.077517Z","iopub.execute_input":"2025-04-14T03:42:33.078076Z","iopub.status.idle":"2025-04-14T03:42:33.352617Z","shell.execute_reply.started":"2025-04-14T03:42:33.078052Z","shell.execute_reply":"2025-04-14T03:42:33.351811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\"\"\"\nfrom torchvision.models import resnext50_32x4d\n\n# 選擇裝置\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 載入較小的 ResNeXt-50 模型\nbackbone_model = resnext50_32x4d(pretrained=True)\n\n# 定義 FPN 用的輸出 channel\nbackbone_model.out_channels = 2048  # ResNeXt-50 的 layer4 輸出\n\n# 包裝成帶有 FPN 的 backbone\nbackbone_with_fpn = BackboneWithFPN(\n    backbone_model,\n    return_layers={'layer1': '0', 'layer2': '1', 'layer3': '2', 'layer4': '3'},\n    in_channels_list=[256, 512, 1024, 2048],\n    out_channels=256,\n    extra_blocks=LastLevelMaxPool()\n)\n\n# 建立 Faster R-CNN 模型\nmodel = FasterRCNN(backbone=backbone_with_fpn, num_classes=11)  # 10 digits + background\nmodel.to(device)\n###\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:28:22.049197Z","iopub.execute_input":"2025-04-13T13:28:22.049567Z","iopub.status.idle":"2025-04-13T13:28:23.957931Z","shell.execute_reply.started":"2025-04-13T13:28:22.049544Z","shell.execute_reply":"2025-04-13T13:28:23.957229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.backbone_utils import BackboneWithFPN\nfrom torchvision.models import resnext50_32x4d\nfrom torchvision.ops import misc as misc_nn_ops\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.ops.feature_pyramid_network import LastLevelMaxPool\n\n# Step 1: Define the model structure (must match the one used in training)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load ResNeXt-50\nbackbone = resnext50_32x4d(pretrained=False)\nbackbone.out_channels = 2048\n\n# Wrap in FPN\nbackbone_with_fpn = BackboneWithFPN(\n    backbone,\n    return_layers={\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"},\n    in_channels_list=[256, 512, 1024, 2048],\n    out_channels=256,\n    extra_blocks=LastLevelMaxPool()\n)\n\n# Build the FasterRCNN model (must match num_classes from training)\nmodel = FasterRCNN(backbone=backbone_with_fpn, num_classes=11)  # 10 digits + background\ncheckpoint = torch.load('/kaggle/input/version1/pytorch/default/1/fasterrcnn_epoch2.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DigitDataset(Dataset):\n    def __init__(self, root_dir, annotation_path, transforms=None):\n        self.root_dir = root_dir\n        self.transforms = transforms\n        with open(annotation_path) as f:\n            coco = json.load(f)\n\n        # 建立 image_id -> file_name 對照\n        self.image_info = {img[\"id\"]: img[\"file_name\"] for img in coco[\"images\"]}\n\n        # 建立 image_id -> list of annotations\n        self.annotations = {}\n        for ann in coco[\"annotations\"]:\n            img_id = ann[\"image_id\"]\n            if img_id not in self.annotations:\n                self.annotations[img_id] = []\n            self.annotations[img_id].append(ann)\n\n        self.ids = list(self.image_info.keys())\n\n    def __getitem__(self, idx):\n        image_id = self.ids[idx]\n        img_path = os.path.join(self.root_dir, self.image_info[image_id])\n        img = Image.open(img_path).convert(\"RGB\")\n\n        boxes = []\n        labels = []\n        for ann in self.annotations.get(image_id, []):\n            boxes.append(ann[\"bbox\"])  # COCO 格式: [x_min, y_min, w, h]\n            labels.append(ann[\"category_id\"])\n\n        boxes = torch.tensor(boxes, dtype=torch.float32)\n        labels = torch.tensor(labels, dtype=torch.int64)\n\n        # Faster R-CNN expects [x_min, y_min, x_max, y_max]\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"image_id\": torch.tensor([image_id])\n        }\n\n        if self.transforms:\n            img = self.transforms(img)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ids)\nprint(\"define dataset finish\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:28:27.505640Z","iopub.execute_input":"2025-04-13T13:28:27.506375Z","iopub.status.idle":"2025-04-13T13:28:27.515253Z","shell.execute_reply.started":"2025-04-13T13:28:27.506347Z","shell.execute_reply":"2025-04-13T13:28:27.514510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_transform(train):\n    transforms = []\n    transforms.append(T.ToTensor())\n    # 可以加更多 Augmentation（僅限訓練時）\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)\n\n# collate_fn for variable-size targets\ndef collate_fn(batch):\n    return tuple(zip(*batch))\nprint(\"define transform finish\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:28:29.941659Z","iopub.execute_input":"2025-04-13T13:28:29.942281Z","iopub.status.idle":"2025-04-13T13:28:29.947009Z","shell.execute_reply.started":"2025-04-13T13:28:29.942257Z","shell.execute_reply":"2025-04-13T13:28:29.946221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport json\ntrain_dataset = DigitDataset(\n    root_dir='/kaggle/input/dataset/nycu-hw2-data/train',\n    annotation_path='/kaggle/input/dataset/nycu-hw2-data/train.json',\n    transforms=get_transform(train=False)\n)\n\nvalid_dataset = DigitDataset(\n    root_dir='/kaggle/input/dataset/nycu-hw2-data/valid',\n    annotation_path='/kaggle/input/dataset/nycu-hw2-data/valid.json',\n    transforms=get_transform(train=False)\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\nvalid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\nprint(\"dataset ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:28:32.592587Z","iopub.execute_input":"2025-04-13T13:28:32.592881Z","iopub.status.idle":"2025-04-13T13:28:33.066988Z","shell.execute_reply.started":"2025-04-13T13:28:32.592859Z","shell.execute_reply":"2025-04-13T13:28:33.066205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for images, targets in train_loader:\n    print(type(images), len(images), type(images[0]))\n    print(type(targets), len(targets), type(targets[0]))\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:28:34.894241Z","iopub.execute_input":"2025-04-13T13:28:34.894696Z","iopub.status.idle":"2025-04-13T13:28:34.998718Z","shell.execute_reply.started":"2025-04-13T13:28:34.894672Z","shell.execute_reply":"2025-04-13T13:28:34.998032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for images, targets in train_loader:\n    print(\"✅ Images:\")\n    print(type(images), len(images))\n    print(images[0].shape, images[0].dtype)\n    \n    print(\"\\n✅ Targets:\")\n    print(type(targets), len(targets))\n    print(targets[0].keys())\n    print(\"boxes:\", targets[0][\"boxes\"].shape, targets[0][\"boxes\"].dtype)\n    print(\"labels:\", targets[0][\"labels\"].shape, targets[0][\"labels\"].dtype)\n    print(\"image_id:\", targets[0][\"image_id\"], targets[0][\"image_id\"].shape)\n    \n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:28:37.161271Z","iopub.execute_input":"2025-04-13T13:28:37.161962Z","iopub.status.idle":"2025-04-13T13:28:37.200092Z","shell.execute_reply.started":"2025-04-13T13:28:37.161933Z","shell.execute_reply":"2025-04-13T13:28:37.199535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\ndef get_optimizer(model, lr=0.005, weight_decay=1e-4):\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n    return optimizer\n\ndef get_scheduler(optimizer):\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n    return scheduler\nprint(\"define opt sch finish\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:28:39.607176Z","iopub.execute_input":"2025-04-13T13:28:39.607457Z","iopub.status.idle":"2025-04-13T13:28:39.613149Z","shell.execute_reply.started":"2025-04-13T13:28:39.607437Z","shell.execute_reply":"2025-04-13T13:28:39.612450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10):\n    model.train()\n    running_loss = 0.0\n    progress = tqdm(data_loader, desc=f\"Epoch {epoch}\")\n    count = 0\n    for images, targets in progress:\n        count +=1\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        if count % 100 == 0:\n            print(count)\n        running_loss += losses.item()\n        progress.set_postfix(loss=losses.item())\n\n    epoch_loss = running_loss / len(data_loader)\n    return epoch_loss\nprint(\"define train one epoch finish\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:28:41.798499Z","iopub.execute_input":"2025-04-13T13:28:41.799050Z","iopub.status.idle":"2025-04-13T13:28:41.805220Z","shell.execute_reply.started":"2025-04-13T13:28:41.799025Z","shell.execute_reply":"2025-04-13T13:28:41.804408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, epoch, path=\"checkpoint\"):\n    os.makedirs(path, exist_ok=True)\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }, os.path.join(path, f\"fasterrcnn_epoch2{epoch}.pth\"))\nprint(\"define save_chechpoint finish\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:28:43.839802Z","iopub.execute_input":"2025-04-13T13:28:43.840646Z","iopub.status.idle":"2025-04-13T13:28:43.847538Z","shell.execute_reply.started":"2025-04-13T13:28:43.840609Z","shell.execute_reply":"2025-04-13T13:28:43.846508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.ops import box_iou\nfrom collections import defaultdict\nimport numpy as np\nimport pycocotools.mask as mask_util\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nimport json\nimport tempfile\nprint(\"import 2 finish\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:28:45.492585Z","iopub.execute_input":"2025-04-13T13:28:45.492872Z","iopub.status.idle":"2025-04-13T13:28:45.520503Z","shell.execute_reply.started":"2025-04-13T13:28:45.492850Z","shell.execute_reply":"2025-04-13T13:28:45.519792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate_on_validation(model, valid_loader, device):\n    model.eval()\n    running_loss = 0.0\n\n    for images, targets in tqdm(valid_loader, desc=\"Validating\"):\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        # 計算 loss，但不進行 backward\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        running_loss += losses.item()\n\n    val_loss = running_loss / len(valid_loader)\n    return val_loss\nprint(\"define eval finish\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:28:47.520670Z","iopub.execute_input":"2025-04-13T13:28:47.521201Z","iopub.status.idle":"2025-04-13T13:28:47.526886Z","shell.execute_reply.started":"2025-04-13T13:28:47.521178Z","shell.execute_reply":"2025-04-13T13:28:47.526192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_map(model, valid_loader, annotation_file, device):\n    model.eval()\n    results = []\n    image_ids = []\n\n    print(\"🔍 Running inference for mAP...\")\n    for images, targets in tqdm(valid_loader, desc=\"Evaluating mAP\"):\n        images = list(img.to(device) for img in images)\n        outputs = model(images)\n\n        for target, output in zip(targets, outputs):\n            image_id = int(target[\"image_id\"].item())\n            image_ids.append(image_id)\n\n            boxes = output[\"boxes\"].detach().cpu().numpy()\n            scores = output[\"scores\"].detach().cpu().numpy()\n            labels = output[\"labels\"].detach().cpu().numpy()\n\n            for box, score, label in zip(boxes, scores, labels):\n                x_min, y_min, x_max, y_max = box\n                width = x_max - x_min\n                height = y_max - y_min\n                results.append({\n                    \"image_id\": image_id,\n                    \"category_id\": int(label),\n                    \"bbox\": [x_min, y_min, width, height],\n                    \"score\": float(score)\n                })\n\n    # 存成 pred.json 暫存檔\n    with tempfile.NamedTemporaryFile(suffix='.json', delete=False) as tmp_pred_file:\n        json.dump(results, tmp_pred_file)\n        pred_path = tmp_pred_file.name\n\n    coco_gt = COCO(annotation_file)\n    coco_dt = coco_gt.loadRes(pred_path)\n\n    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    coco_eval.summarize()\n\n    return coco_eval.stats[0]  # mAP at IoU=0.5:0.95\nprint(\"define eval map finish\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:28:49.656686Z","iopub.execute_input":"2025-04-13T13:28:49.657271Z","iopub.status.idle":"2025-04-13T13:28:49.664840Z","shell.execute_reply.started":"2025-04-13T13:28:49.657249Z","shell.execute_reply":"2025-04-13T13:28:49.664051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, valid_loader, device, num_epochs=2, save_every=1, val_json_path='/kaggle/input/dataset/nycu-hw2-data/valid.json'):\n    optimizer = get_optimizer(model)\n    scheduler = get_scheduler(optimizer)\n\n    for epoch in range(1, num_epochs + 1):\n        print(f\"\\n🟢 Epoch {epoch}/{num_epochs}\")\n        train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n        if epoch % save_every == 0:\n            save_checkpoint(model, optimizer, epoch)\n        #val_loss = evaluate_on_validation(model, valid_loader, device)\n        #map_score = evaluate_map(model, valid_loader, val_json_path, device)\n        scheduler.step()\n\n        #print(f\"[Epoch {epoch}]  Train Loss: {train_loss:.4f} |  Val Loss: {val_loss:.4f} |  mAP: {map_score:.4f}\")\nprint(\"define train finish\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:28:54.310937Z","iopub.execute_input":"2025-04-13T13:28:54.311631Z","iopub.status.idle":"2025-04-13T13:28:54.316912Z","shell.execute_reply.started":"2025-04-13T13:28:54.311609Z","shell.execute_reply":"2025-04-13T13:28:54.316221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_model(model, train_loader, valid_loader, device, num_epochs=2, save_every=1, val_json_path=\"/kaggle/input/dataset/nycu-hw2-data/valid.json\")\nprint(\"train finish\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T13:26:31.716807Z","iopub.execute_input":"2025-04-13T13:26:31.717089Z","iopub.status.idle":"2025-04-13T13:26:31.822495Z","shell.execute_reply.started":"2025-04-13T13:26:31.717061Z","shell.execute_reply":"2025-04-13T13:26:31.821668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nimport torch\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.backbone_utils import BackboneWithFPN\nfrom torchvision.models import resnext50_32x4d\nfrom torchvision.ops import misc as misc_nn_ops\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.ops.feature_pyramid_network import LastLevelMaxPool\n\n# Step 1: Define the model structure (must match the one used in training)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load ResNeXt-50\nbackbone = resnext50_32x4d(pretrained=False)\nbackbone.out_channels = 2048\n\n# Wrap in FPN\nbackbone_with_fpn = BackboneWithFPN(\n    backbone,\n    return_layers={\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"},\n    in_channels_list=[256, 512, 1024, 2048],\n    out_channels=256,\n    extra_blocks=LastLevelMaxPool()\n)\n\n# Build the FasterRCNN model (must match num_classes from training)\nmodel = FasterRCNN(backbone=backbone_with_fpn, num_classes=11)  # 10 digits + background\ncheckpoint = torch.load('/kaggle/input/version1/pytorch/default/1/fasterrcnn_epoch2.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:40:41.181207Z","iopub.execute_input":"2025-04-14T03:40:41.182158Z","iopub.status.idle":"2025-04-14T03:40:42.166576Z","shell.execute_reply.started":"2025-04-14T03:40:41.182112Z","shell.execute_reply":"2025-04-14T03:40:42.165950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\ndef save_predictions(model, data_loader, device, output_file=\"pred.json\", score_threshold=0.3):\n    model.eval()\n    results = []\n\n    with torch.no_grad():\n        for images, targets in tqdm(data_loader, desc=\"Running inference\"):\n            images = list(img.to(device) for img in images)\n            outputs = model(images)\n\n            for i, output in enumerate(outputs):\n                image_id = targets[i]\n                for box, label, score in zip(output[\"boxes\"], output[\"labels\"], output[\"scores\"]):\n                    if score < score_threshold:\n                        continue\n\n                    box = box.tolist()\n                    x_min, y_min, x_max, y_max = box\n                    width = x_max - x_min\n                    height = y_max - y_min\n\n                    results.append({\n                        \"image_id\": image_id,\n                        \"bbox\": [x_min, y_min, width, height],\n                        \"score\": score.item(),\n                        \"category_id\": label.item()\n                    })\n\n    # 儲存成 JSON 並加上換行與縮排\n    with open(output_file, \"w\") as f:\n        json.dump(results, f, indent=4)\n\n    print(f\"✅ Saved {len(results)} predictions to {output_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:47:02.786442Z","iopub.execute_input":"2025-04-14T03:47:02.786770Z","iopub.status.idle":"2025-04-14T03:47:02.794036Z","shell.execute_reply.started":"2025-04-14T03:47:02.786725Z","shell.execute_reply":"2025-04-14T03:47:02.793385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TestDataset(torch.utils.data.Dataset):\n    def __init__(self, image_folder, transform=None):\n        self.image_folder = image_folder\n        self.image_paths = sorted(list(image_folder.glob(\"*.png\")))  # or png\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n\n        image_id = int(image_path.stem)  # assuming image name is like '123.jpg'\n        return image, image_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:41:16.889723Z","iopub.execute_input":"2025-04-14T03:41:16.890279Z","iopub.status.idle":"2025-04-14T03:41:16.895264Z","shell.execute_reply.started":"2025-04-14T03:41:16.890254Z","shell.execute_reply":"2025-04-14T03:41:16.894546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms\nfrom pathlib import Path\n\ntest_transform = transforms.Compose([\n    transforms.ToTensor()\n])\n\ntest_dataset = TestDataset(Path(\"/kaggle/input/dataset/nycu-hw2-data/test\"), transform=test_transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: list(zip(*x)))\nprint(\"test data ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:41:41.969055Z","iopub.execute_input":"2025-04-14T03:41:41.969707Z","iopub.status.idle":"2025-04-14T03:41:42.224133Z","shell.execute_reply.started":"2025-04-14T03:41:41.969678Z","shell.execute_reply":"2025-04-14T03:41:42.223381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel.to(device)\nsave_predictions(model, test_loader, device, output_file=\"pred.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T03:47:08.382090Z","iopub.execute_input":"2025-04-14T03:47:08.382380Z","iopub.status.idle":"2025-04-14T04:17:40.098420Z","shell.execute_reply.started":"2025-04-14T03:47:08.382359Z","shell.execute_reply":"2025-04-14T04:17:40.097723Z"}},"outputs":[],"execution_count":null}]}